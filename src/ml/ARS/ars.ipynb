{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 2018\n",
    "import sys;\n",
    "import os;\n",
    "import numpy as np;\n",
    "\n",
    "# Setting Hyper Parameters\n",
    "class Hp():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nb_steps = 1000\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.nosie = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = ''\n",
    "\n",
    "\n",
    "#  Normalise the states\n",
    "class Normalizer():\n",
    "\n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(np_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x- self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "\n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else :\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "\n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "\n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) *d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r)\n",
    "        \n",
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0\n",
    "    sum_rewards = 0\n",
    "    while not done  and num_plays < hp.episode_lenght:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards\n",
    "    \n",
    "    \n",
    "# Training the AI\n",
    "\n",
    "def train(env, policy, normalizer, hp):\n",
    "    for step in range(hp.nb_steps):\n",
    "        print(\"\")\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "print(\"Hello \")\n",
    "# print(hp.nb_steps)\n",
    "\n",
    "# explore()  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
